{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0807b62c",
   "metadata": {},
   "source": [
    "##### Entry point of PySpark :  Spark Context\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparksc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a108a",
   "metadata": {},
   "source": [
    "##### Inspect SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve SparkContext version\n",
    "sc.version   \n",
    "\n",
    "#Retrieve Python version\n",
    "sc.pythonVer\n",
    "\n",
    "#Master URL to connect to\n",
    "sc.master\n",
    "\n",
    "#Path where Spark is installed on worker nodes\n",
    "str(sc.sparkHome)\n",
    "\n",
    "#Retrieve name of the Spark User running SparkContext\n",
    "str(sc.sparkUser())\n",
    "\n",
    "#Return application name\n",
    "sc.appName\n",
    "\n",
    "#Retrieve application ID\n",
    "sc.applicationId\n",
    "\n",
    "#Return default level of parallelism\n",
    "sc.defaultParallelism\n",
    "\n",
    "#Default minimum number of partitions for RDDs\n",
    "sc.defaultMinPartitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6fdfb",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411755b",
   "metadata": {},
   "source": [
    "#### Load data from external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from text file : Before that import the file nad copy the path\n",
    "rdd3 = sc.textFile(\"/FileStore/tables/001_Wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rading data from CSV file\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"/FileStore/tables/avocado.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa42de",
   "metadata": {},
   "source": [
    "### Parallelize Collections\n",
    "\n",
    "#### Use the parallelize method to convert a local collection to an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('b',2),('c',2)])\n",
    "\n",
    "rdd2 = sc.parallelize([('a',2),('d' ,1),('b',1)])\n",
    "\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "\n",
    "rdd4 = sc.parallelize([('a',['x' ,'y' ,'z' ]),('b' ,['p' ,'r' ])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1c649",
   "metadata": {},
   "source": [
    "#### Basic Information"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65598329",
   "metadata": {},
   "source": [
    "#List the number of partitions\n",
    "rdd.getNumPartitions()\n",
    "\n",
    "#Count RDD instances 3\n",
    "rdd.count()\n",
    "\n",
    "#Count RDD instances by key\n",
    "rdd.countByKey()\n",
    "\n",
    "#Count RDD instances by value\n",
    "rdd.countByValue()\n",
    "\n",
    "#Return (key,value) pairs as a dictionary\n",
    "rdd.collectAsMap()\n",
    "\n",
    "#Sum of RDD elements 4950\n",
    "rdd3.sum()\n",
    "\n",
    "#Check whether RDD is empty\n",
    "sc.parallelize([]).isEmpty()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403b12fc",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c437d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum value of RDD elements\n",
    "rdd3.max()\n",
    "\n",
    "#Minimum value of RDD elements\n",
    "rdd3.min()\n",
    "\n",
    "\n",
    "#Mean value of RDD elements\n",
    "rdd3.mean()\n",
    " \n",
    "#Standard deviation of RDD elements\n",
    "rdd3.stdev()\n",
    "\n",
    "#Compute variance of RDD elements\n",
    "rdd3.variance()\n",
    "\n",
    "\n",
    "#Compute histogram by bins\n",
    "rdd3.histogram(3)\n",
    "\n",
    "#Summary statistics (count, mean, stdev, max & min)\n",
    "rdd3.stats()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfcd077",
   "metadata": {},
   "source": [
    "#### Applying Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d11887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use case: Use map when you need to transform each element of the RDD independently.\n",
    "#Apply a function to each RDD element \n",
    "rdd.map(lambda x: x+(x[1],x[0])).collect()\n",
    "\n",
    "# python \n",
    "al = [('a',7),('b',2),('c',2)]\n",
    "list(map(lambda x: x+(x[1],x[0]),al))\n",
    "\n",
    "\n",
    "# Use case: Use flatMap when each input element should map to zero or more output elements.\n",
    "#Apply a function to each RDD element and flatten the result\n",
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0]))\n",
    "\n",
    "# Use case: Use flatMapValues when working with key-value RDDs and you want to transform the values while keeping the keys intact, potentially expanding each value into multiple values.\n",
    "#Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys\n",
    "rdd4.flatMapValues(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d9671",
   "metadata": {},
   "source": [
    "## Function of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b60bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show()\n",
    "data = [(1,'Ram'),(2,'Sam'),(3,'Emily'),(4,'Andy')]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# df.show(2, truncate=3)\n",
    "# df.show(4, truncate=3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76038a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printSchema  : helps to know the datatype of column in a table\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700d492",
   "metadata": {},
   "source": [
    "#### withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'Ram','3000'),(2,'Sam','4000'),(3,'Emily','3500'),(4,'Andy','6000')]\n",
    "schema = ['id', 'name','salary']\n",
    "\n",
    "df1 = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "\n",
    "# Casting the datatype of a column\n",
    "from pyspark.sql.functions import col\n",
    "df2 = df1.withColumn(colName='salary',col=col('salary').cast('Integer'))\n",
    "df2.show()\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying column data\n",
    "df3 = df2.withColumn('salary', col('salary')*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column with value\n",
    "from pyspark.sql.functions import lit\n",
    "df4 = df2.withColumn('country', lit('India'))\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy a column\n",
    "df5 = df4.withColumn('copysalary', col('salary'))\n",
    "df5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4388b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column name\n",
    "df6 =df5.withColumnRenamed('copysalary', 'kopySalary')\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54d5bf",
   "metadata": {},
   "source": [
    "#### structType()  and  structField() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88908fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "data = [(1,'Ram',4000),(2,'Sam',6000),(3,'Emily',10000),(4,'Andy',6200)]\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", IntegerType())\n",
    "])\n",
    "\n",
    "df8 = spark.createDataFrame(data, schema)\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using struct type\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "data = [(1,('Ram','Smith'),4000),(2,('Sam','Smith'),6000),(3,('Tim','Smith'),10000),(4,('Paul','Smith'),6200)]\n",
    "\n",
    "structName = StructType([\n",
    "                     StructField(\"firstname\", StringType()),\n",
    "                      StructField(\"lastname\", StringType()),\n",
    "])\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", structName),\n",
    "    StructField(\"salary\", IntegerType())\n",
    "])\n",
    "\n",
    "df9 = spark.createDataFrame(data, schema)\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7671d2",
   "metadata": {},
   "source": [
    "#### ArrayType Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, ArrayType\n",
    "\n",
    "data = [('abc',[1,2,3]),('xyz',[15,23,23]),('dac',[11,22,31])]\n",
    "\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"numbers\", ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "df10 = spark.createDataFrame(data, schema)\n",
    "df10.show()\n",
    "\n",
    "# we can use Array index to fetch value\n",
    "df10.withColumn('firstNumber', df10.numbers[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef80983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,array\n",
    "\n",
    "df1.withColumns('numbers', array(col('id'),col('salary')))\n",
    "\n",
    "data = [(1,'Ram','3000'),(2,'Sam','4000'),(3,'Emily','3500'),(4,'Andy','6000')]\n",
    "schema = ['id', 'name','salary']\n",
    "\n",
    "df1 = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# Another use : creating a new Array type column with present column value\n",
    "df12 = df11.withColumn('numbers', array(col('id'),col('salary')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709481fb",
   "metadata": {},
   "source": [
    "#### understanding explode(), split(), array(), array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c49314",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'ram',['python','pyspark']),(2,'sam',['sql','aws'])]\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df12 = spark.createDataFrame(data,schema)\n",
    "display(df12)\n",
    "\n",
    "# explode()  : it will create a row for each element in Array\n",
    "from pyspark.sql.functions import col,explode\n",
    "\n",
    "df13 = df12.withColumn('oneSkill',explode(col('skills')))\n",
    "df13.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'ram','python,pyspark'),(2,'sam','sql,aws')]\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df12 = spark.createDataFrame(data,schema)\n",
    "display(df12)\n",
    "\n",
    "# split() : it will split string to an Array\n",
    "from pyspark.sql.functions import col,split\n",
    "df13 = df12.withColumn('oneSkillArray',split(col('skills'),','))\n",
    "df13.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'ram','python','pyspark'),(2,'sam','sql','aws')]\n",
    "schema = ['id', 'name', 'prim_skills','sec_skills']\n",
    "\n",
    "\n",
    "df13 = spark.createDataFrame(data,schema)\n",
    "display(df13)\n",
    "\n",
    "# array()  : to create a column for holding array datatype using column values\n",
    "from pyspark.sql.functions import col,array \n",
    "df14 = df13.withColumn('oneSkillArray',array(col('prim_skills'),col('sec_skills')))\n",
    "df14.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'ram',['python','pyspark']),(2,'sam',['sql','aws'])]\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df14 = spark.createDataFrame(data,schema)\n",
    "display(df14)\n",
    "\n",
    "# array_contains  : return True if element is present : N.B: It is case sensitive\n",
    "from pyspark.sql.functions import split,col,array, array_contains \n",
    "df15 = df14.withColumn('HasPythonSkill',array_contains(col('skills'),'python'))\n",
    "display(df15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566f8ac",
   "metadata": {},
   "source": [
    "### MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10962ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of MapType \n",
    "data = [('ram',{'hair':'black','eyes':'black','skill':'java'}),('sam',{'hair':'red','eyes':'blue','skill':'python'})]\n",
    "schema = ['name','properties']\n",
    "\n",
    "df15 = spark.createDataFrame(data,schema)\n",
    "display(df15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using structType() defining schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType,MapType\n",
    "data = [('ram',{'hair':'black','eyes':'black','skill':'java'}),('sam',{'hair':'red','eyes':'blue','skill':'python'})]\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"properties\", MapType(StringType(),StringType()))\n",
    "])\n",
    "\n",
    "\n",
    "df16 = spark.createDataFrame(data,schema)\n",
    "display(df16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 2\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType,MapType\n",
    "data = [('ram',{'hair':'black','eyes':'black','skill':'java','age':32}),('sam',{'hair':'red','eyes':'blue','skill':'python','age':23})]\n",
    "\n",
    "# Define the inner schema for the dictionary\n",
    "inner_schema = StructType([\n",
    "    StructField(\"hair\", StringType(), True),\n",
    "    StructField(\"eyes\", StringType(), True),\n",
    "    StructField(\"skill\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define the outer schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"attributes\", inner_schema, True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'age' by extracting it from the 'attributes' column\n",
    "df18 = df17.withColumn(\"age\", col(\"attributes.age\"))\n",
    "df18.show()\n",
    "\n",
    "df19 = df18.withColumn(\"hair\", df18.attributes['hair'])\n",
    "df19.show()\n",
    "\n",
    "df19 = df18.withColumn(\"hair\", df18.attributes.getItem('hair'))\n",
    "df19.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode()  on map keys : it explodes the dictionary into seperate rows for each keys\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df1 = df16.select('name','properties',explode(df16.properties))\n",
    "df1.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_keys() : fetches all keys and store in array\n",
    "from pyspark.sql.functions import map_keys\n",
    "df2 = df16.withColumn('keys',map_keys(df16.properties))\n",
    "df2.show()\n",
    "\n",
    "\n",
    "# map_values() : fetches all keys and store in array\n",
    "from pyspark.sql.functions import map_values\n",
    "df2 = df16.withColumn('values',map_values(df16.properties))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d793bc",
   "metadata": {},
   "source": [
    "## Row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can Row object by using named argument or create custom row like class \n",
    "# Row is represent as record/ row in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row('ram', 2000)\n",
    "row1 = Row(name = 'sam', salary =3000)\n",
    "print(row[0] +' ' + str(row[1]))\n",
    "print(row1.name + '  ' + str(row1.salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe using row\n",
    "from pyspark.sql import Row\n",
    "\n",
    "row1 = Row(name = 'sam', salary =3000)\n",
    "row2 = Row(name = 'ram', salary =4000)\n",
    "\n",
    "data = [row1,row2]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Row like class\n",
    "\n",
    "Person = Row('name','salary')\n",
    "p1 = Person('ram',3000)\n",
    "p2 = Person('sam',2000)\n",
    "\n",
    "print(p1.name + ' '+ str(p1.salary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a358c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data in dataframe represent as Row\n",
    "data = [p1,p2]\n",
    "df1 = spark.createDataFrame(data)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9392e",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Creating nested struct using Row()\n",
    "    data = [Row(name='ram', prop=Row(hair='black',eyes='blue',skill='java')), \n",
    "        Row(name='sam', prop=Row(hair='blue',eyes='black',skill='python'))]\n",
    "\n",
    "df3 = spark.createDataFrame(data)\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a73041",
   "metadata": {},
   "source": [
    "## Column class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lit() function to fill value to a column\n",
    "from pyspark.sql.functions import lit\n",
    "df9 = df8.withColumn('newCol', lit('Hello'))\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch column\n",
    "df8.select(df8.name).show()\n",
    "df8.select(df8['name']).show()\n",
    "from pyspark.sql.functions import col\n",
    "df8.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch column vlue if struct type\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "data = [(1,'Ram',4000,('black','blue')),(2,'Sam',6000,('blue','red'))]\n",
    "\n",
    "propertyType = StructType([\n",
    "    StructField(\"hair\", StringType()),\n",
    "    StructField(\"eyes\", StringType()),\n",
    "\n",
    "])\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", IntegerType()),\n",
    "    StructField(\"properties\", propertyType)\n",
    "])\n",
    "\n",
    "df8 = spark.createDataFrame(data, schema)\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aebd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.select(df8.properties.hair).show()\n",
    "df8.select(df8['properties.hair']).show()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df8.select(col('properties.hair')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a41afa",
   "metadata": {},
   "source": [
    "## when  AND otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865307ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'Ram','M',3000),(2,'Sam','M',6000),(3,'Emily','F',7000),(4,'Andy','',6500)]\n",
    "schema = ['id', 'name','gender','salary']\n",
    "\n",
    "df1 = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df3 =df1.select(df1.id,df1.name,\n",
    "                when(df1.gender=='M','male')\n",
    "                .when(df1.gender=='F','female')\n",
    "                .otherwise('unknown').alias('genderCol')\n",
    "                )\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c682a",
   "metadata": {},
   "source": [
    "## Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2232bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alias() : to rename column \n",
    "df1.select(df1.id.alias('emp_id'), df1.name,df1.gender,df1.salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asc()\n",
    "df1.sort(df1.name.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#desc()\n",
    "df1.sort(df1.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast()\n",
    "df3 = df1.select(df1.id,df1.name,df1.gender,df1.salary.cast('int'))\n",
    "df3.printSchema()\n",
    "\n",
    "# other available cast type\n",
    "Integer: cast(\"int\")\n",
    "String: cast(\"string\")\n",
    "Float: cast(\"float\")\n",
    "Double: cast(\"double\")\n",
    "Date: to_date(column, \"format\")\n",
    "Timestamp: to_timestamp(column, \"format\")\n",
    "Boolean: cast(\"boolean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All String data\n",
    "\n",
    "data = [(\"1\", \"2024-06-26\", \"true\", \"123.45\")]\n",
    "schema = [\"integer_col\", \"date_col\", \"boolean_col\", \"float_col\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumn(\"date_col\", to_date(col(\"date_col\"), \"yyyy-MM-dd\"))\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumn(\"date_col\", to_timestamp(col(\"date_col\"), \"yyyy-MM-dd\"))\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# multiple Casting in one line\n",
    "df = df.withColumn(\"integer_col\", col(\"integer_col\").cast(\"int\")) \\\n",
    "       .withColumn(\"date_col\", to_date(col(\"date_col\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"boolean_col\", col(\"boolean_col\").cast(\"boolean\")) \\\n",
    "       .withColumn(\"float_col\", col(\"float_col\").cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like operator : Same as SQL : Case sensitive\n",
    "\n",
    "df1.filter(df1.name.like('E%')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd730ca5",
   "metadata": {},
   "source": [
    "### filter()  where() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.filter(df1.gender== 'M').show()\n",
    "\n",
    "df1.filter(\"gender =='F' \").show()\n",
    "\n",
    "df1.where(\"gender =='M' \").show()\n",
    "\n",
    "\n",
    "# Multiple condition\n",
    "df1.filter((df1.gender == 'F') & (df1.salary == 7000)).show()\n",
    "\n",
    "# Other way\n",
    "\n",
    "# Correct usage with filter and col functions\n",
    "from pyspark.sql.functions import col\n",
    "df1.filter((col(\"gender\") == 'F') & (col(\"salary\") == 7000)).show()\n",
    "\n",
    "# Using a SQL expression directly in filter\n",
    "df1.filter(\"gender = 'F' AND salary = 7000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07528c03",
   "metadata": {},
   "source": [
    "## distinct()  and   dropDuplicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd41ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct   : to fetch unique rows\n",
    "df2.distinct().show()\n",
    "\n",
    "# dropDuplicates  : N.B if nothing is passed the works as distinct\n",
    "df2.dropDuplicates().show()\n",
    "\n",
    "# passing columns \n",
    "df2.dropDuplicates(['gender']).show()\n",
    "\n",
    "# if matches in both column then row is dropped\n",
    "df2.dropDuplicates(['gender','salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d46985",
   "metadata": {},
   "source": [
    "## orderBy()  and sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cf47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'Ram','M',3000,'IT'),(2,'Sam','M',6000,'HR'),(2,'Sam','M',6000,'TE'),(3,'Emily','F',7000,'IT'),(4,'Andy','M',6000,'HR')]\n",
    "schema = ['id', 'name','gender','salary','department']\n",
    "\n",
    "df2 = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orderBy and ort are interchangeable\n",
    "\n",
    "df2.sort('department').show()\n",
    "df2.sort(df2.department).show()\n",
    "\n",
    "df2.orderBy('department').show()\n",
    "df2.orderBy(df2.department).show()\n",
    "\n",
    "df2.orderBy(df2.department,df2.id).show()\n",
    "df2.orderBy(df2.department,df2.id.desc()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c848b",
   "metadata": {},
   "source": [
    "### Union and unionAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(1,'Ram','M',3000),(2,'Sam','M',6000),(3,'Emily','F',7000),(4,'Andy','M',6000)]\n",
    "schema = ['id', 'name','gender','salary']\n",
    "\n",
    "data2 = [(5,'Ramesh','M',4000),(6,'Samy','M',6000),(7,'Emma','F',7000),(8,'SAndy','M',6000)]\n",
    "schema = ['id', 'name','gender','salary']\n",
    "\n",
    "\n",
    "df2 = spark.createDataFrame(data=data1, schema=schema)\n",
    "df3 = spark.createDataFrame(data=data2, schema=schema)\n",
    "\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c17c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B : duplicate row is not delected in union() and unionAll()\n",
    "newdf = df2.union(df3)\n",
    "newdf = df2.unionAll(df3)\n",
    "newdf.show()\n",
    "\n",
    "# to remove duplicate use distinct()\n",
    "newdf.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161e17b",
   "metadata": {},
   "source": [
    "## unionByName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e517a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two df when schema is different and put Null value to it\n",
    "df4 = df2.unionByName(df3,allowMissingColumns=True)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ef53d",
   "metadata": {},
   "source": [
    "### groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0baee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.groupBy('department').count()\n",
    "df3 = df2.groupBy(df2.department).count()\n",
    "df3 = df2.groupBy('department').min('salary')\n",
    "df3 = df2.groupBy('department').max('salary')\n",
    "df3 = df2.groupBy('department','gender').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b93c9",
   "metadata": {},
   "source": [
    "### agg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,max,min\n",
    "df2.groupBy('department').agg(count('*').alias('EmpCount'),min('salary').alias('minSal'),max('salary').alias('maxSal')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb27d36",
   "metadata": {},
   "source": [
    "## select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch column\n",
    "df4.select(df4.id,df4.name,df4.gender).show()\n",
    "df4.select('id','name','gender').show()\n",
    "df4.select(df4['id'],df4['name']).show()\n",
    "df4.select(['id','name']).show()\n",
    "df4.select('*').show()\n",
    "df4.select([col for col in df4.columns]).show()\n",
    "from pyspark.sql.functions import col\n",
    "df8.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87755933",
   "metadata": {},
   "source": [
    "### Join in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaeb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample employee data\n",
    "employee_data = [\n",
    "    (1, \"Alice\", 101),\n",
    "    (2, \"Bob\", 102),\n",
    "    (3, \"Cathy\", 101),\n",
    "    (4, \"David\", 103),\n",
    "    (5, \"Eva\", 102),\n",
    "    (6, \"Emily\", 106)\n",
    "]\n",
    "\n",
    "# Sample department data\n",
    "department_data = [\n",
    "    (101, \"HR\"),\n",
    "    (102, \"Engineering\"),\n",
    "    (103, \"Marketing\"),\n",
    "    (104, \"Sales\"),\n",
    "    (105, \"Payroll\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "employee_df = spark.createDataFrame(employee_data, [\"EmployeeID\", \"Name\", \"DepartmentID\"])\n",
    "department_df = spark.createDataFrame(department_data, [\"DepartmentID\", \"DepartmentName\"])\n",
    "\n",
    "# Show DataFrames\n",
    "employee_df.show()\n",
    "department_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join : matching row fetched from both column\n",
    "inner_join_df = employee_df.join(department_df, employee_df.DepartmentID == department_df.DepartmentID, \"inner\")\n",
    "inner_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join : all matched row plus unmatched from left table\n",
    "left_join_df = employee_df.join(department_df, employee_df.DepartmentID == department_df.DepartmentID, \"left\")\n",
    "left_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# right join : all matched row plus unmatched from right table\n",
    "right_join_df = employee_df.join(department_df, employee_df.DepartmentID == department_df.DepartmentID, \"right\")\n",
    "right_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9663c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full outer join : all matched row plus unmatched from left and right table\n",
    "outer_join_df = employee_df.join(department_df, employee_df.DepartmentID == department_df.DepartmentID, \"outer\")\n",
    "outer_join_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917be27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross join : Returns the Cartesian product of the two DataFrames.\n",
    "cross_join_df = employee_df.crossJoin(department_df)\n",
    "cross_join_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac651e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left semi : fetching matched row(inner join) but show only left table\n",
    "left_semi_join_df = employee_df.join(department_df, employee_df.DepartmentID == department_df.DepartmentID, \"left_semi\")\n",
    "left_semi_join_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left anti : opposite of left semi : i.e fetchin non matching row and shows left table\n",
    "left_anti_join_df = employee_df.join(department_df, employee_df.DepartmentID == department_df.DepartmentID, \"left_anti\")\n",
    "left_anti_join_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self join : join with same data frame\n",
    "employee_data_with_manager = [\n",
    "    (1, \"Alice\", 101, None),\n",
    "    (2, \"Bob\", 102, 1),\n",
    "    (3, \"Cathy\", 101, 1),\n",
    "    (4, \"David\", 103, 2),\n",
    "    (5, \"Eva\", 102, 2)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "employee_with_manager_df = spark.createDataFrame(employee_data_with_manager, [\"EmployeeID\", \"Name\", \"DepartmentID\", \"ManagerID\"])\n",
    "employee_with_manager_df.show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "self_join_df = employee_with_manager_df.alias(\"emp\").join(\n",
    "    employee_with_manager_df.alias(\"mgr\"),\n",
    "    col(\"emp.ManagerID\") == col(\"mgr.EmployeeID\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"emp.EmployeeID\").alias(\"EmployeeID\"),\n",
    "    col(\"emp.Name\").alias(\"EmployeeName\"),\n",
    "    col(\"mgr.Name\").alias(\"ManagerName\")\n",
    ")\n",
    "\n",
    "self_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b672a",
   "metadata": {},
   "source": [
    "## pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdd8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample employee data\n",
    "employee_data = [\n",
    "    (1, \"Alice\", \"HR\", 5000,\"male\"),\n",
    "    (2, \"Bob\", \"Engineering\", 6000,\"male\"),\n",
    "    (3, \"Cathy\", \"HR\", 5500,\"female\"),\n",
    "    (4, \"David\", \"Engineering\", 7000,\"male\"),\n",
    "    (5, \"Eva\", \"Marketing\", 6000,\"female\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data, [\"EmployeeID\", \"Name\", \"Department\", \"Salary\",\"Gender\"])\n",
    "\n",
    "# Show DataFrame\n",
    "employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b73d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Pivot the DataFrame\n",
    "pivot_df = employee_df.groupBy(\"Department\") \\\n",
    "    .pivot(\"Name\") \\\n",
    "    .sum(\"Salary\")\n",
    "\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cab7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Pivot the DataFrame\n",
    "nopivot_df = employee_df.groupBy(\"Department\",\"Gender\").count()\n",
    "\n",
    "# Pivot the DataFrame\n",
    "pivot_df = employee_df.groupBy(\"Department\").pivot(\"Gender\").count()\n",
    "nopivot_df.show()\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea64d3",
   "metadata": {},
   "source": [
    "### Unpivot dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d03417",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"HR\", 10, 15), (\"IT\", 20, 5), (\"Sales\", 30, 25)]\n",
    "columns = [\"department\", \"male\", \"female\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Unpivot DataFrame : for column name no single quote\n",
    "unpivoted_df = df.selectExpr(\"department\", \"stack(2, 'M', male, 'F', female) as (gender, count)\")\n",
    "\n",
    "# Unpivot DataFrame : for column name no single quote\n",
    "unpivoted_df = df.select(\"department\", expr(\"stack(2, 'M', male, 'F', female) as (gender, count)\"))\n",
    "\n",
    "# Show the result\n",
    "unpivoted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152669be",
   "metadata": {},
   "source": [
    "#### fill() and fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51282d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", \"F\", 5000, \"HR\"),\n",
    "    (2, \"Bob\", \"M\", None, \"IT\"),\n",
    "    (3, \"Carol\", \"F\", 7000, None),\n",
    "    (4, \"Dave\", None, 8000, \"IT\"),\n",
    "    (5, None, \"M\", 9000, \"Sales\")\n",
    "]\n",
    "columns = [\"id\", \"name\", \"gender\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in specific columns\n",
    "filled_df = df.fillna({\"name\": \"Unknown\", \"salary\": 0, \"department\": \"Unknown\"})\n",
    "\n",
    "# Show the result\n",
    "filled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc300142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values across all columns with a single value\n",
    "filled_df_all = df.fillna(\"Unknown\")\n",
    "\n",
    "# Show the result\n",
    "filled_df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values across all columns with a single value\n",
    "filled_df_all = df.na.fill(\"Unknown\",\"department\")\n",
    "\n",
    "# Show the result\n",
    "filled_df_all.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e0fa5",
   "metadata": {},
   "source": [
    "### sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15798626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = \"/FileStore/tables/avocado.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df1 = df.sample(fraction=0.1)\n",
    "display(df1)\n",
    "\n",
    "# to get same row use seed\n",
    "df1 = df.sample(fraction=0.1, seed =123)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae1c46",
   "metadata": {},
   "source": [
    "### collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0612f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", \"F\", 5000, \"HR\"),\n",
    "    (2, \"Bob\", \"M\", 6000, \"IT\"),\n",
    "    (3, \"Carol\", \"F\", 7000, \"IT\"),\n",
    "    (4, \"Dave\", \"M\", 8000, \"HR\"),\n",
    "    (5, \"Eve\", \"F\", 9000, \"Sales\")\n",
    "]\n",
    "columns = [\"id\", \"name\", \"gender\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ae96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the DataFrame to the driver as a list of Row objects\n",
    "collected_data = df.collect()\n",
    "\n",
    "# Print the collected data\n",
    "for row in collected_data:\n",
    "    print(row)\n",
    "\n",
    "    \n",
    "print(collected_data)\n",
    "print(collected_data[0])\n",
    "print(collected_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afb425",
   "metadata": {},
   "source": [
    "#### transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", \"F\", 5000, \"HR\"),\n",
    "    (2, \"Bob\", \"M\", 6000, \"IT\"),\n",
    "    (3, \"Carol\", \"F\", 7000, \"IT\"),\n",
    "    (4, \"Dave\", \"M\", 8000, \"HR\"),\n",
    "    (5, \"Eve\", \"F\", 9000, \"Sales\")\n",
    "]\n",
    "columns = [\"id\", \"name\", \"gender\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom transformation function\n",
    "def add_bonus(df):\n",
    "    return df.withColumn(\"bonus\", when(col(\"salary\") < 7000, col(\"salary\") * 0.10).otherwise(col(\"salary\") * 0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38627bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom transformation function\n",
    "def convertToUpper(df):\n",
    "    return df.withColumn(\"name\", upper(\"name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation using the transform method\n",
    "transformed_df = df.transform(add_bonus)\n",
    "\n",
    "# Show the result\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform()   : applied on column of array type\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56add338",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, transform, upper\n",
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", [\"Python\", \"Java\", \"Scala\"]),\n",
    "    (\"Bob\", [\"C++\", \"Python\", \"JavaScript\"]),\n",
    "    (\"Carol\", [\"Java\", \"C#\", \"Python\"]),\n",
    "    (\"Dave\", [\"JavaScript\", \"HTML\", \"CSS\"]),\n",
    "    (\"Eve\", [\"Scala\", \"Python\", \"Go\"])\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"programmingskills\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the transformation to convert each programming skill to uppercase\n",
    "transformed_df = df.withColumn(\"programmingskills\", transform(col(\"programmingskills\"), lambda x: upper(x)))\n",
    "\n",
    "# Show the result\n",
    "transformed_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21cbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, transform, upper\n",
    "\n",
    "# Define a custom transformation function\n",
    "def convertArrayToUpper(x):\n",
    "    return upper(x)\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", [\"Python\", \"Java\", \"Scala\"]),\n",
    "    (\"Bob\", [\"C++\", \"Python\", \"JavaScript\"]),\n",
    "    (\"Carol\", [\"Java\", \"C#\", \"Python\"]),\n",
    "    (\"Dave\", [\"JavaScript\", \"HTML\", \"CSS\"]),\n",
    "    (\"Eve\", [\"Scala\", \"Python\", \"Go\"])\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"programmingskills\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the transformation to convert each programming skill to uppercase\n",
    "transformed_df = df.withColumn(\"programmingskills\", transform(\"programmingskills\", convertArrayToUpper))\n",
    "\n",
    "# Show the result\n",
    "transformed_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3efcf8",
   "metadata": {},
   "source": [
    "### createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 5000),\n",
    "    (2, \"Bob\", 6000),\n",
    "    (3, \"Carol\", 7000),\n",
    "    (4, \"Dave\", 8000),\n",
    "    (5, \"Eve\", 9000)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c68e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"employee\")\n",
    "\n",
    "# Perform an SQL query on the temporary view\n",
    "result_df = spark.sql(\"SELECT * FROM employee WHERE salary > 6000\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "OR\n",
    "\n",
    "%sql\n",
    "select id,upper(name) as Name from employee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0008092",
   "metadata": {},
   "source": [
    "### createOrReplaceGlobalTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceGlobalTempView(\"employee\")\n",
    "\n",
    "\n",
    "%sql\n",
    "SELECT * from global_temp.employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f484394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view tables from current session\n",
    "spark.catalog.listTables(spark.catalog.currentDatabase())\n",
    "\n",
    "\n",
    "# to view global temp tables\n",
    "spark.catalog.listTables('global_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75523b",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf , col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 5000, 500),\n",
    "    (2, \"Bob\", 6000, 600),\n",
    "    (3, \"Carol\", 7000, 700),\n",
    "    (4, \"Dave\", 8000, 800),\n",
    "    (5, \"Eve\", 9000, 900)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"salary\", \"bonus\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a UDF to calculate total compensation\n",
    "def calculate_total_compensation(salary, bonus):\n",
    "    return salary + bonus\n",
    "\n",
    "# Register the UDF\n",
    "calculate_total_compensation_udf = udf(calculate_total_compensation, IntegerType())\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df_with_total_compensation = df.withColumn(\"total_compensation\", calculate_total_compensation_udf(df.salary, df.bonus))\n",
    "\n",
    "# Show the result\n",
    "df_with_total_compensation.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to calculate total compensation using the @udf decorator\n",
    "@udf(returnType=IntegerType())\n",
    "def calculate_total_compensation(salary, bonus):\n",
    "    return salary + bonus\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df_with_total_compensation = df.withColumn(\"total_compensation\", calculate_total_compensation(col(\"salary\"), col(\"bonus\")))\n",
    "\n",
    "# Show the result\n",
    "df_with_total_compensation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e32b57",
   "metadata": {},
   "source": [
    "#### udf.register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_compensation(salary, bonus):\n",
    "    return salary + bonus\n",
    "\n",
    "spark.udf.register(name='TotalPaySQL', f=calculate_total_compensation, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "Select id, TotalPaySQL(salary,bonus) as totPay from emps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053ed6d",
   "metadata": {},
   "source": [
    "## Understanding RDD and some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Alice\", 5000, 500),\n",
    "    (2, \"Bob\", 6000, 600),\n",
    "    (3, \"Carol\", 7000, 700),\n",
    "    (4, \"Dave\", 8000, 800),\n",
    "    (5, \"Eve\", 9000, 900)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"salary\", \"bonus\"]\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d119f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from Python list\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show RDD\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17cb186",
   "metadata": {},
   "source": [
    "## map() example:  it iterates over each element and perform some action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase salary by 10%\n",
    "def increase_salary(record):\n",
    "    id, name, salary, bonus = record\n",
    "    return (id, name, salary * 1.10, bonus)\n",
    "\n",
    "\n",
    "rdd_with_increased_salary = rdd.map(increase_salary)\n",
    "\n",
    "print(\"map() result:\", rdd_with_increased_salary.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bf5f0",
   "metadata": {},
   "source": [
    "## flatMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap() example: Split names into characters\n",
    "def split_name_into_chars(record):\n",
    "    id, name, salary, bonus = record\n",
    "    return list(name)\n",
    "\n",
    "rdd_with_split_names = rdd.flatMap(split_name_into_chars)\n",
    "print(\"flatMap() result:\", rdd_with_split_names.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc8648",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, to_json, json_tuple, schema_of_json, get_json_object\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define JSON schema\n",
    "json_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read JSON file into DataFrame\n",
    "df = spark.read.json(\"path/to/json/file\")\n",
    "\n",
    "# Parse JSON string column into StructType\n",
    "df_parsed = df.withColumn(\"data\", from_json(col(\"value\"), json_schema))\n",
    "\n",
    "# Extract JSON object based on path\n",
    "df_extracted = df_parsed.withColumn(\"name\", get_json_object(col(\"value\"), \"$.name\"))\n",
    "\n",
    "# Convert StructType column back to JSON string\n",
    "df_json = df_extracted.withColumn(\"json_string\", to_json(col(\"data\")))\n",
    "\n",
    "# Write DataFrame to JSON file\n",
    "df_json.write.json(\"path/to/output/json\")\n",
    "\n",
    "# Show the result\n",
    "df_json.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806798ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_json, from_json, get_json_object, json_tuple\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ('ram', {'hair': 'black', 'eyes': 'black', 'skill': 'java'}),\n",
    "    ('sam', {'hair': 'red', 'eyes': 'blue', 'skill': 'python'})\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"attributes\"])\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to JSON format\n",
    "df = df.withColumn(\"attributes_json\", to_json(col(\"attributes\")))\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da183643",
   "metadata": {},
   "source": [
    "### from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for parsing the JSON column\n",
    "schema = StructType([\n",
    "    StructField(\"hair\", StringType(), True),\n",
    "    StructField(\"eyes\", StringType(), True),\n",
    "    StructField(\"skill\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON column\n",
    "df_parsed = df.withColumn(\"parsed_attributes\", from_json(col(\"attributes_json\"), schema))\n",
    "\n",
    "df_parsed.show(truncate=False)\n",
    "df_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd04902",
   "metadata": {},
   "source": [
    "#### get_json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49238ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific fields using get_json_object\n",
    "df_with_fields = df_parsed.withColumn(\"hair\", get_json_object(col(\"attributes_json\"), \"$.hair\")) \\\n",
    "                          .withColumn(\"eyes\", get_json_object(col(\"attributes_json\"), \"$.eyes\")) \\\n",
    "                          .withColumn(\"skill\", get_json_object(col(\"attributes_json\"), \"$.skill\"))\n",
    "\n",
    "df_with_fields.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baff97",
   "metadata": {},
   "source": [
    "## json_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79568f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fields using json_tuple\n",
    "df_with_tuple = df_with_fields.withColumn(\"hair_tuple\", json_tuple(col(\"attributes_json\"), \"hair\")) \\\n",
    "                              .withColumn(\"eyes_tuple\", json_tuple(col(\"attributes_json\"), \"eyes\")) \\\n",
    "                              .withColumn(\"skill_tuple\", json_tuple(col(\"attributes_json\"), \"skill\"))\n",
    "\n",
    "# Show the result\n",
    "df_with_tuple.select(\"name\", \"attributes_json\", \"hair\", \"eyes\", \"skill\", \"hair_tuple\", \"eyes_tuple\", \"skill_tuple\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c29728",
   "metadata": {},
   "source": [
    "#### Date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_date, current_timestamp, date_format, year, month, dayofmonth, hour, minute, second, datediff, add_months, date_add, date_sub, to_date, to_timestamp\n",
    "\n",
    "# Sample data\n",
    "data = [(\"2024-07-01\", \"2024-01-01 12:34:56\")]\n",
    "df = spark.createDataFrame(data, [\"date_str\", \"timestamp_str\"])\n",
    "\n",
    "# Convert string columns to date and timestamp\n",
    "df = df.withColumn(\"date\", to_date(col(\"date_str\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"timestamp\", to_timestamp(col(\"timestamp_str\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Apply date functions\n",
    "df = df.withColumn(\"current_date\", current_date()) \\\n",
    "       .withColumn(\"current_timestamp\", current_timestamp()) \\\n",
    "       .withColumn(\"formatted_date\", date_format(col(\"date\"), \"MM/dd/yyyy\")) \\\n",
    "       .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "       .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "       .withColumn(\"day_of_month\", dayofmonth(col(\"date\"))) \\\n",
    "       .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"minute\", minute(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"second\", second(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"days_diff\", datediff(col(\"current_date\"), col(\"date\"))) \\\n",
    "       .withColumn(\"add_months\", add_months(col(\"date\"), 2)) \\\n",
    "       .withColumn(\"date_add\", date_add(col(\"date\"), 10)) \\\n",
    "       .withColumn(\"date_sub\", date_sub(col(\"date\"), 10))\n",
    "\n",
    "# Show the result\n",
    "df.select(\"date_str\", \"timestamp_str\", \"date\", \"timestamp\", \"current_date\", \"current_timestamp\", \"formatted_date\", \"year\", \"month\", \"day_of_month\", \"hour\", \"minute\", \"second\", \"days_diff\", \"add_months\", \"date_add\", \"date_sub\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b25fc2",
   "metadata": {},
   "source": [
    "#### approx_count_distinct(),avg(),collect_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, avg, collect_list\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", 1, 50),\n",
    "    (\"Bob\", 2, 60),\n",
    "    (\"Alice\", 3, 50),\n",
    "    (\"Bob\", 4, 70),\n",
    "    (\"Alice\", 5, 80),\n",
    "    (\"Bob\", 6, 80)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"id\", \"score\"])\n",
    "\n",
    "# Group by name and apply aggregation functions\n",
    "result_df = df.groupBy(\"name\").agg(\n",
    "    approx_count_distinct(\"id\").alias(\"approx_distinct_id\"),\n",
    "    avg(\"score\").alias(\"avg_score\"),\n",
    "    collect_list(\"score\").alias(\"scores_list\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a65f9",
   "metadata": {},
   "source": [
    "### row_number(), rank(),dense_rank()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "202cfb2d",
   "metadata": {},
   "source": [
    "row_number(): Assigns a unique sequential number to each row within a partition, starting from 1.\n",
    "\n",
    "rank(): Assigns a unique number to each row within a partition based on the ordering of rows, with gaps in the ranking sequence for tied values.\n",
    "\n",
    "dense_rank(): Similar to rank(), but without gaps in the ranking sequence for tied values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655bdf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, rank, dense_rank\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", \"Math\", 85),\n",
    "    (\"Alice\", \"English\", 78),\n",
    "    (\"Alice\", \"Science\", 92),\n",
    "    (\"Bob\", \"Math\", 95),\n",
    "    (\"Bob\", \"English\", 89),\n",
    "    (\"Bob\", \"Science\", 72),\n",
    "    (\"Charlie\", \"Math\", 70),\n",
    "    (\"Charlie\", \"English\", 65),\n",
    "    (\"Charlie\", \"Science\", 80)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"subject\", \"score\"])\n",
    "\n",
    "# Define a window specification\n",
    "windowSpec = Window.partitionBy(\"name\").orderBy(col(\"score\").desc())\n",
    "\n",
    "# Apply row_number, rank, and dense_rank\n",
    "df_with_ranks = df.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    "                  .withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"dense_rank\", dense_rank().over(windowSpec))\n",
    "\n",
    "# Show the result\n",
    "df_with_ranks.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
